{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "from torchaudio.utils import download_asset\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import tensorflow as tf\n",
    "from tensorflow import Graph\n",
    "import json\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imgdata(Dataset):\n",
    "    def __init__(self, df, data_path, transform = None):\n",
    "        self.df = df\n",
    "        self.transform = transform\n",
    "        self.data_path = data_path\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        # Absolute file path of the audio file - concatenate the audio directory with\n",
    "        # the relative path\n",
    "        img_file = self.df.loc[idx, 'Path']\n",
    "        # Get the Class ID\n",
    "        class_id = self.df.loc[idx, 'ID']\n",
    "        img = mpimg.imread(img_file)\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, class_id\n",
    "    \n",
    "    \n",
    "class imgClassifier (nn.Module):\n",
    "    # ----------------------------\n",
    "    # Build the model architecture\n",
    "    # ----------------------------\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_layers = []\n",
    "\n",
    "        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        init.kaiming_normal_(self.conv1.weight, a=0.1)\n",
    "        self.conv1.bias.data.zero_()\n",
    "        conv_layers += [self.conv1, self.relu1, self.bn1]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        init.kaiming_normal_(self.conv2.weight, a=0.1)\n",
    "        self.conv2.bias.data.zero_()\n",
    "        conv_layers += [self.conv2, self.relu2, self.bn2]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        init.kaiming_normal_(self.conv3.weight, a=0.1)\n",
    "        self.conv3.bias.data.zero_()\n",
    "        conv_layers += [self.conv3, self.relu3, self.bn3]\n",
    "\n",
    "        # Second Convolution Block\n",
    "        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        init.kaiming_normal_(self.conv4.weight, a=0.1)\n",
    "        self.conv4.bias.data.zero_()\n",
    "        conv_layers += [self.conv4, self.relu4, self.bn4]\n",
    "\n",
    "        # Linear Classifier\n",
    "        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n",
    "        self.lin = nn.Linear(in_features=64, out_features=450)\n",
    "\n",
    "        # Wrap the Convolutional Blocks\n",
    "        self.conv = nn.Sequential(*conv_layers)\n",
    " \n",
    "    # ----------------------------\n",
    "    # Forward pass computations\n",
    "    # ----------------------------\n",
    "    def forward(self, x):\n",
    "        # Run the convolutional blocks\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Adaptive pool and flatten for input to linear layer\n",
    "        x = self.ap(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        # Linear layer\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Final output\n",
    "        return x\n",
    "    \n",
    "   \n",
    "    def preprocess(model, path_to_image, path_to_dummy):\n",
    "        \n",
    "        MEAN = np.array([0.485, 0.456, 0.406])\n",
    "        STD = np.array([0.229, 0.224, 0.225])\n",
    "        \n",
    "        IMG_path = [path_to_image, path_to_dummy]\n",
    "        df_test = pd.DataFrame( {\"Path\":IMG_path, \"ID\":[0, 1]} )\n",
    "        \n",
    "        test_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(MEAN,STD), transforms.Resize((224, 224))])  \n",
    "\n",
    "        testDS = imgdata(df_test, IMG_path, test_transform)\n",
    "        test_dl = torch.utils.data.DataLoader(testDS, batch_size=64, shuffle=False)\n",
    "        \n",
    "        return test_dl\n",
    "    \n",
    "    def predict(model, preprocessed_image):\n",
    "        \n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "\n",
    "        # Disable gradient updates\n",
    "        with torch.no_grad():\n",
    "            for data in preprocessed_image:\n",
    "                # Get the input features and target labels, and put them on the GPU\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "                # Normalize the inputs\n",
    "                inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "                inputs = (inputs - inputs_m) / inputs_s\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                # print(outputs)\n",
    "\n",
    "                    # Get the predicted class with the highest score\n",
    "                _, prediction = torch.max(outputs,1)\n",
    "                    # Count of predictions that matched the target label\n",
    "                return int(prediction[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276\n"
     ]
    }
   ],
   "source": [
    "from joblib import load\n",
    "\n",
    "model_img = load('C:\\\\Users\\\\Tee\\Desktop\\\\venv_Django\\\\SE_Project_\\\\SE_Site\\\\models_ML\\\\model_128_0.05', 'rb')\n",
    "\n",
    "img_used = imgClassifier.preprocess(model_img, \"D:\\\\Downloads\\\\foto2.jpg\", \"D:\\\\Downloads\\\\181269802.jpg\")\n",
    "\n",
    "result = imgClassifier.predict(model_img, img_used)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 20]\n"
     ]
    }
   ],
   "source": [
    "alist = [0, 1, 20]\n",
    "\n",
    "x = [i for i in alist]\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "203362965eee0c80b390fddf918a7130fd57bdc69a0e6b12d40f260644c97e9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
